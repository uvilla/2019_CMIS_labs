{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal-dual Newton's method for TV Denoising\n",
    "\n",
    "We present the Primal-dual Newton's method to solver the Total Variation denoising problem\n",
    "\n",
    "$$ \\min_{m \\in \\mathcal{M}} J(m) = \\min_{m} \\int_\\Omega (m -d)^2 d{\\boldsymbol{x}} + \\alpha \\int_\\Omega |\\nabla m |_\\beta d{\\boldsymbol{x}}, $$\n",
    "\n",
    "where $d = d({\\boldsymbol{x}})$ is the data (noisy image), $\\alpha > 0$ is the regularization parameter, and $|\\nabla m|_\\beta = \\sqrt{\\|\\nabla m^2\\| + \\beta}$ with $\\beta > 0$ leads to a differentiable approximation of the TV functional.\n",
    "\n",
    "The variational formulation of the first order optimality conditions reads\n",
    "\n",
    "*Find $m \\in \\mathcal{M}$ such that:* \n",
    "\n",
    "$$ \\int_\\Omega ( m - d)\\tilde{m} d{\\boldsymbol{x}} + \\alpha \\int_\\Omega \\frac{\\nabla m \\cdot \\nabla\\tilde{m}}{|\\nabla m|_\\beta} d{\\boldsymbol{x}} = 0 \\quad \\forall \\tilde{m} \\in \\mathcal{M}. $$\n",
    "\n",
    "By standard techniques, we can show that if $m$ is smooth enough the above variational form is equivalent to solving the boundary value problem\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "-\\alpha \\nabla \\cdot \\left( \\frac{\\nabla m}{| \\nabla m |_\\beta} \\right) + m = d, & \\text{in } \\Omega\\\\\n",
    "\\alpha \\left( \\frac{\\nabla m}{| \\nabla m |_\\beta} \\right) \\cdot {\\boldsymbol n} = 0, & \\text{on } \\partial\\Omega,\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "where ${\\boldsymbol n}$ is the unit outward normal vector to $\\partial\\Omega$.\n",
    "\n",
    "A key observation is that the nonlinearity in the term\n",
    "$$ \\boldsymbol{w} = \\frac{\\nabla m}{| \\nabla m |_\\beta} $$\n",
    "is the cause of the slow convergence of the Newton's method, althought the variable $\\boldsymbol{w}$ itself is usually smooth since it represents the unit normal vector to the level sets of $m$.\n",
    "\n",
    "The primal-dual Newton's method then explicitly introduce the variable $\\boldsymbol{w}$ in the first optimality conditions leading to the system of equations\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "-\\alpha \\nabla \\cdot \\boldsymbol{w} + m = d, & \\text{in } \\Omega\\\\\n",
    "\\boldsymbol{w}|\\nabla m|_\\beta - \\nabla m = 0, & \\text{in } \\Omega,\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "equipped with the boundary conditions $\\nabla m \\cdot \\boldsymbol{n} = \\boldsymbol{w} \\cdot \\boldsymbol{n} = 0$ on $\\partial \\Omega$.\n",
    "\n",
    "In the primal-dual method, the Newton direction $(\\hat{m}_k, \\boldsymbol{\\hat{w}}_k)$ is obtained by linearizing the system of nonlinear equation around a point $(m_k, \\boldsymbol{w}_k)$ and solving\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "- \\alpha \\nabla \\cdot & I \\\\\n",
    "| \\nabla m_k |_\\beta & -\\left( I - \\frac{\\boldsymbol{w_k}\\otimes \\nabla m_k}{| \\nabla m_k |_\\beta}\\right) \\nabla \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \\boldsymbol{\\hat{w}}_k \\\\ \\hat{m}_k \\end{bmatrix} = \n",
    "- \\begin{bmatrix} \n",
    "-\\alpha \\nabla \\cdot \\boldsymbol{w}_k + m_k - d \\\\\n",
    "\\boldsymbol{w_k}|\\nabla m_k|_\\beta - \\nabla m_k\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The solution is then updated as\n",
    "\n",
    "$$\n",
    "m_{k+1} = m_{k} + \\alpha_m \\hat{m}_k, \\quad \\boldsymbol{w}_{k+1} = \\boldsymbol{w}_k +\\alpha_w \\boldsymbol{\\hat{w}}_k,\n",
    "$$ \n",
    "\n",
    "where $\\alpha_m$ is chosen to ensure sufficient descent of $J(m)$ and $\\alpha_w$ is chosen to ensure $\\boldsymbol{w}_{k+1} \\cdot \\boldsymbol{w}_{k+1} \\leq 1$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python imports\n",
    "\n",
    "We import the following libraries:\n",
    "\n",
    "- `__future__`, which allows compatibility between different version of Python (Python 2 v.s. Python 3)\n",
    "- `math`, which contains several mathematical functions\n",
    "- `matplotlib, numpy, scipy`, three libraries that together allow similar functionalities to matlab\n",
    "- `dolfin`, which allows us to discretize and solve variational problems using the finite element method\n",
    "- `hippylib`, the extesible framework I created to solve inverse problems in Python\n",
    "\n",
    "Finally, we import the `logging` library to silence most of the output produced by `dolfin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import dolfin as dl\n",
    "\n",
    "from hippylib import nb\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "dl.set_log_active(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The image class\n",
    "\n",
    "The `Image` class is an extension of the `dolfin` class `Expression`, and allows us to convert from an image represented as a 2d array of pixels values to an object that `dolfin` can use to construct variational forms.\n",
    "\n",
    "The constructor of the `Image` class takes the following inputs:\n",
    "\n",
    "- `Lx`, `Ly`: the dimension of the image in physical units\n",
    "- `data`: a numpy array of size Nx pixels by Ny pixels.\n",
    "\n",
    "The `eval` function takes as input a point `x` in the physical coordinate systems an returns in the array `values` the value of the image in the pixels corresponding to point `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image(dl.Expression):\n",
    "    def __init__(self, Lx, Ly, data, **kwargs):\n",
    "        self.data = data\n",
    "        self.hx = Lx/float(data.shape[1]-1)\n",
    "        self.hy = Ly/float(data.shape[0]-1)\n",
    "        \n",
    "    def eval(self, values, x):\n",
    "        j = int(math.floor(x[0]/self.hx))\n",
    "        i = int(math.floor(x[1]/self.hy))\n",
    "        values[0] = self.data[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geometry, true image, and data.\n",
    "\n",
    "1. Read the true image from file, store the pixel values in `data`\n",
    "\n",
    "2. Define the width `Lx` and height `Ly` of the region of iterest such that the aspect ratio of the image is preserved.\n",
    "\n",
    "3. Generate a triangulation (pixelation) `mesh` of the region of interest.\n",
    "\n",
    "4. Define the finite element spaces:\n",
    "   - `Vm` of piecewise linear function on the elements of `mesh` representing the space of discretized images;\n",
    "   - `Vw` of piecewise constant vector functions on the elements of `mesh` representing the space of the normalized gradients $\\boldsymbol{w}$\n",
    "   - `Vwnom` of piecewise constant functions on the elements of `mesh` representing the space of the $l^2$ of the normalized gradients $\\boldsymbol{w}$\n",
    "\n",
    "5. Interpolate the true image in the discrete space `Vm`. Call this interpolation `m_true`.\n",
    "\n",
    "6. Corrupt the true image with i.i.d. Gaussian noise ($\\sigma^2 = 0.09$) and interpolated the noisy image in the discrete space `Vm`. Call this interpolation `d`.\n",
    "\n",
    "7. Visualize the true image `m_true` and the noisy image `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('circles.mat')['im']\n",
    "\n",
    "Lx = float(data.shape[1])/float(data.shape[0])\n",
    "Ly = 1.\n",
    "\n",
    "nx, ny = [256, 256]\n",
    "  \n",
    "mesh = dl.RectangleMesh(dl.Point(0,0),dl.Point(Lx,Ly),nx, ny)\n",
    "Vm = dl.FunctionSpace(mesh, \"Lagrange\",1)\n",
    "Vw = dl.VectorFunctionSpace(mesh, \"DG\",0)\n",
    "Vwnorm = dl.FunctionSpace(mesh, \"DG\",0)\n",
    "\n",
    "trueImage = Image(Lx,Ly,data,degree = 1)\n",
    "m_true  = dl.interpolate(trueImage, Vm)\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "noise_std_dev = .3\n",
    "noise = noise_std_dev*np.random.randn(data.shape[0], data.shape[1])\n",
    "noisyImage = Image(Lx,Ly,data+noise, degree = 1)\n",
    "d = dl.interpolate(noisyImage, Vm)\n",
    "\n",
    "# Get min/max of noisy image, so that we can show all plots in the same scale\n",
    "vmin = np.min(d.vector().get_local())\n",
    "vmax = np.max(d.vector().get_local())\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(m_true, subplot_loc=121, mytitle=\"True Image\", vmin=vmin, vmax = vmax)\n",
    "nb.plot(d, subplot_loc=122, mytitle=\"Noisy Image\", vmin=vmin, vmax = vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Total Variation denoising\n",
    "\n",
    "The class `PDTVDenosing` defines the cost functional and its first & second variations for the primal dual formulation of the Total Variation denoising problem.\n",
    "\n",
    "Specifically, the method `cost` implements the cost functional\n",
    "$$ \\mathcal{J}(m) = \\frac{1}{2}\\int_\\Omega (m - d)^2 \\; d\\boldsymbol{x} + \\frac{\\alpha}{2}\\int_\\Omega | \\nabla m |_\\beta d\\boldsymbol{x}, $$\n",
    "where $\\alpha$ is the amount of regularization and $\\beta$ is a small pertubation to ensure differentiability of the total variation functional.\n",
    "\n",
    "The method `grad_m` implements the first variation of $\\mathcal{J}$ w.r.t. $m$\n",
    "$$ \\delta_m \\mathcal{J}(m, \\tilde{m}) = \\int_\\Omega (m - m_0)\\tilde{m}  \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{| \\nabla m |_\\beta}\\nabla m \\cdot \\nabla \\tilde{m}  d\\boldsymbol{x}. $$\n",
    "\n",
    "The method `Hessian` implements the action of primal-dual second variation of $\\mathcal{J}$ w.r.t. $m$ in the direction $\\hat{m}$:\n",
    "$$ \\int_\\Omega \\tilde{m} \\hat{m} \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{| \\nabla m |_\\beta} \\left[ \\left( I - A(m,w)\\right) \\nabla \\tilde{m}\\right] \\cdot \\nabla \\hat{m} d\\boldsymbol{x}, $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ A(m,w) = \\frac{1}{2} \\boldsymbol{w} \\otimes \\frac{\\nabla m}{|\\nabla m|_\\beta} + \\frac{1}{2} \\frac{\\nabla m}{|\\nabla m|_\\beta}\\otimes \\boldsymbol{w}. $$\n",
    "\n",
    "Finaly, the method `compute_w_hat` computes the primal dual Newton update for $\\boldsymbol{w}$,\n",
    "\n",
    "$$ \\hat{\\boldsymbol{w}} = \\frac{1}{|\\nabla m|_\\beta}\\left(I - A(m,w) \\right)\\nabla \\hat{m} - \\boldsymbol{w} + \\frac{\\nabla m}{|\\nabla m|_\\beta}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDTVDenoising:\n",
    "    def __init__(self, Vm, Vw, Vwnorm, d, alpha, beta):\n",
    "        self.alpha   = dl.Constant(alpha)\n",
    "        self.beta    = dl.Constant(beta)\n",
    "        self.d       = d\n",
    "        self.m_tilde  = dl.TestFunction(Vm)\n",
    "        self.m_hat = dl.TrialFunction(Vm)\n",
    "        \n",
    "        self.Vm = Vm\n",
    "        self.Vw = Vw\n",
    "        self.Vwnorm = Vwnorm\n",
    "        \n",
    "    def cost_reg(self, m):\n",
    "        return dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)*dl.dx\n",
    "    \n",
    "    def cost_misfit(self, m):\n",
    "        return dl.Constant(.5)*dl.inner(m-self.d, m - self.d)*dl.dx\n",
    "        \n",
    "    def cost(self, m):        \n",
    "        return self.cost_misfit(m) + self.alpha*self.cost_reg(m)\n",
    "        \n",
    "    def grad_m(self, m):    \n",
    "        grad_ls = dl.inner(self.m_tilde, m - self.d)*dl.dx        \n",
    "        TVm = dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)\n",
    "        grad_tv = dl.Constant(1.)/TVm*dl.inner(dl.grad(m), dl.grad(self.m_tilde))*dl.dx\n",
    "        \n",
    "        grad = grad_ls + self.alpha*grad_tv\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "    def Hessian(self,m, w):\n",
    "        H_ls = dl.inner(self.m_tilde, self.m_hat)*dl.dx\n",
    "        \n",
    "        TVm = dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)\n",
    "        A = dl.Constant(1.)/TVm * (dl.Identity(2) \n",
    "                                   - dl.Constant(.5)*dl.outer(w, dl.grad(m)/TVm )\n",
    "                                   - dl.Constant(.5)*dl.outer(dl.grad(m)/TVm, w ) )\n",
    "        \n",
    "        H_tv = dl.inner(A*dl.grad(self.m_tilde), dl.grad(self.m_hat))*dl.dx\n",
    "         \n",
    "        H = H_ls + self.alpha*H_tv\n",
    "                                   \n",
    "        return H\n",
    "    \n",
    "    def compute_w_hat(self, m, w, m_hat):\n",
    "        TVm = dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)\n",
    "        A = dl.Constant(1.)/TVm * (dl.Identity(2) \n",
    "                                   - dl.Constant(.5)*dl.outer(w, dl.grad(m)/TVm )\n",
    "                                   - dl.Constant(.5)*dl.outer(dl.grad(m)/TVm, w ) )\n",
    "        \n",
    "        expression = A*dl.grad(m_hat) - w + dl.grad(m)/TVm\n",
    "        \n",
    "        return dl.project(expression, self.Vw)\n",
    "    \n",
    "    def wnorm(self, w):\n",
    "        return dl.inner(w,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Primal dual solution of Total Variation regularized denoising problem\n",
    "\n",
    "The `PDNewton` function computes the primal dual solution of the total variation regularized denoising problem using the inexact Newton conjugate gradient algorithm with:\n",
    "\n",
    "- Eisenstat-Walker conditions to reduce the number of conjugate gradient iterations necessary to compute the Newton'direction $\\hat{m}$.\n",
    "\n",
    "- Backtracking linesearch on $m_{k+1}$ based on Armijo's sufficient descent condition\n",
    "\n",
    "- Backtracking linesearch on $\\boldsymbol{w}_{k+1}$ to ensure $\\boldsymbol{w}_{k+1} \\cdot \\boldsymbol{w}_{k+1} \\leq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDNewton(pdProblem, m, w, parameters):\n",
    "    \n",
    "    termination_reasons = [\n",
    "                           \"Maximum number of Iteration reached\",      #0\n",
    "                           \"Norm of the gradient less than tolerance\", #1\n",
    "                           \"Maximum number of backtracking reached\",   #2\n",
    "                           \"Norm of (g, m_hat) less than tolerance\"       #3\n",
    "                           ]\n",
    "    \n",
    "    rtol          = parameters[\"rel_tolerance\"]\n",
    "    atol          = parameters[\"abs_tolerance\"]\n",
    "    gdm_tol       = parameters[\"gdm_tolerance\"]\n",
    "    max_iter      = parameters[\"max_iter\"]\n",
    "    c_armijo      = parameters[\"c_armijo\"] \n",
    "    max_backtrack = parameters[\"max_backtracking_iter\"]\n",
    "    prt_level     = parameters[\"print_level\"]\n",
    "    cg_coarse_tol = parameters[\"cg_coarse_tolerance\"]\n",
    "        \n",
    "    Jn = dl.assemble( pdProblem.cost(m)   )\n",
    "    gn = dl.assemble( pdProblem.grad_m(m) )\n",
    "    g0_norm = gn.norm(\"l2\")\n",
    "    gn_norm = g0_norm\n",
    "    tol = max(g0_norm*rtol, atol)\n",
    "    \n",
    "    m_hat = dl.Function(pdProblem.Vm)\n",
    "    w_hat = dl.Function(pdProblem.Vw)\n",
    "        \n",
    "    converged = False\n",
    "    reason = 0\n",
    "    total_cg_iter = 0\n",
    "        \n",
    "    if prt_level > 0:\n",
    "        print( \"{0:>3} {1:>15} {2:>15} {3:>15} {4:>15} {5:>15} {6:>7}\".format(\n",
    "                \"It\", \"cost\", \"||g||\", \"(g,m_hat)\", \"alpha_m\", \"tol_cg\", \"cg_it\") )\n",
    "        \n",
    "    for it in range(max_iter):\n",
    "        \n",
    "        # Compute m_hat\n",
    "        Hn = dl.assemble( pdProblem.Hessian(m,w) )\n",
    "        solver = dl.PETScKrylovSolver(\"cg\", \"petsc_amg\")\n",
    "        solver.set_operator(Hn)\n",
    "        solver.parameters[\"nonzero_initial_guess\"] = False\n",
    "        cg_tol = min(cg_coarse_tol, math.sqrt( gn_norm/g0_norm) )\n",
    "        solver.parameters[\"relative_tolerance\"] = cg_tol\n",
    "        lin_it = solver.solve(m_hat.vector(),-gn)   \n",
    "        total_cg_iter += lin_it\n",
    "        \n",
    "        # Compute w_hat\n",
    "        w_hat = pdProblem.compute_w_hat(m, w, m_hat)\n",
    "            \n",
    "        ### Line search for m\n",
    "        mhat_gn = m_hat.vector().inner(gn)\n",
    "            \n",
    "        if(-mhat_gn < gdm_tol):\n",
    "            self.converged=True\n",
    "            self.reason = 3\n",
    "            break\n",
    "        \n",
    "        alpha_m = 1.   \n",
    "        bk_converged = False \n",
    "        for j in range(max_backtrack):\n",
    "            Jnext = dl.assemble( pdProblem.cost(m + dl.Constant(alpha_m)*m_hat) )\n",
    "            if Jnext < Jn + alpha_m*c_armijo*mhat_gn:\n",
    "                Jn = Jnext\n",
    "                bk_converged = True\n",
    "                break\n",
    "                \n",
    "            alpha_m = alpha_m/2.\n",
    "                \n",
    "        if not bk_converged:\n",
    "            self.reason = 2\n",
    "            break\n",
    "            \n",
    "        ### Line search for w\n",
    "        alpha_w = 1\n",
    "        bk_converged = False\n",
    "        for j in range(max_backtrack):\n",
    "            norm_w = dl.project(pdProblem.wnorm(w + dl.Constant(alpha_w)*w_hat), pdProblem.Vwnorm)\n",
    "            if norm_w.vector().norm(\"linf\") <= 1:\n",
    "                bk_converged = True\n",
    "                break\n",
    "            alpha_w = alpha_w/2.\n",
    "        \n",
    "        \n",
    "        ### Update\n",
    "        m.vector().axpy(alpha_m, m_hat.vector())\n",
    "        w.vector().axpy(alpha_w, w_hat.vector())\n",
    "        \n",
    "        gn = dl.assemble( pdProblem.grad_m(m) )\n",
    "        gn_norm = gn.norm(\"l2\")\n",
    "            \n",
    "        if prt_level > 0:\n",
    "            print( \"{0:3d} {1:15e} {2:15e} {3:15e} {4:15e} {5:15e} {6:7d}\".format(\n",
    "                    it, Jn, gn_norm, mhat_gn, alpha_m, cg_tol, lin_it) )\n",
    "                \n",
    "        if gn_norm < tol:\n",
    "            converged = True\n",
    "            reason = 1\n",
    "            break\n",
    "            \n",
    "    final_grad_norm = gn_norm\n",
    "        \n",
    "    if prt_level > -1:\n",
    "        print( termination_reasons[reason] )\n",
    "        if converged:\n",
    "            print( \"Inexact Newton CG converged in \", it, \\\n",
    "                \"nonlinear iterations and \", total_cg_iter, \"linear iterations.\" )\n",
    "        else:\n",
    "            print( \"Inexact Newton CG did NOT converge after \", self.it, \\\n",
    "                \"nonlinear iterations and \", total_cg_iter, \"linear iterations.\")\n",
    "        print (\"Final norm of the gradient\", final_grad_norm)\n",
    "        print (\"Value of the cost functional\", Jn)\n",
    "            \n",
    "    return m, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "The primal dual Newton's methods converges in only 23 iterations compared to the hundreds of iterations we observed in the previous notebook by solving the primal formulation with Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3\n",
    "beta  = 1e-4\n",
    "pdProblem = PDTVDenoising(Vm, Vw, Vwnorm, d, alpha, beta)\n",
    "\n",
    "parameters = {}\n",
    "parameters[\"rel_tolerance\"]         = 1e-6\n",
    "parameters[\"abs_tolerance\"]         = 1e-9\n",
    "parameters[\"gdm_tolerance\"]         = 1e-18\n",
    "parameters[\"max_iter\"]              = 100\n",
    "parameters[\"c_armijo\"]              = 1e-5\n",
    "parameters[\"max_backtracking_iter\"] = 10\n",
    "parameters[\"print_level\"]           = 1\n",
    "parameters[\"cg_coarse_tolerance\"]   = 0.5\n",
    "\n",
    "m0 = dl.Function(Vm)\n",
    "w0 = dl.Function(Vw)\n",
    "\n",
    "m, w = PDNewton(pdProblem, m0, w0, parameters)\n",
    "\n",
    "plt.figure()\n",
    "nb.plot(m, vmin=vmin, vmax = vmax, mytitle=\"alpha = {0:1.2e}\".format(alpha))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Copyright &copy; 2019, Washington University in St. Louis.\n",
    "\n",
    "All Rights reserved.\n",
    "See file COPYRIGHT for details.\n",
    "\n",
    "This file is part of **cmis_labs**, the teaching material for the S2019 BME 493/593 course on *Computational Methods in Imaging Science* at Washington University in St. Louis. Please see [https://uvilla.github.io/cmis_labs](https://uvilla.github.io/cmis_labs) for more information and source code availability.\n",
    "\n",
    "We would like to acknowledge the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1548562, for providing cloud computing resources (Jetstream) for this course through allocation TG-SEE190001."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
