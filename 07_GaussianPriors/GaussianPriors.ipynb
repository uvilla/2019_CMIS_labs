{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian priors in infinite dimensions\n",
    "\n",
    "In this notebook we show how to construct PDE-based priors that lead to well-posed Bayesian inverse problems in infinite dimesions.\n",
    "Specifically, we will consider a Gaussian prior $\\mu_{\\rm prior} \\sim \\mathcal{N}( m_{\\rm prior}, \\mathcal{C}_{\\rm prior} )$, where\n",
    "the covariance operator $\\mathcal{C}_{\\rm prior}$ is defined as the inverse of an elliptic differential operator, i.e.\n",
    "\n",
    "$$ \\mathcal{C}_{\\rm prior} = \\left( \\delta I - \\gamma \\Delta \\right)^{-\\alpha}, $$\n",
    "\n",
    "equipped with homogeneous Neumann, Dirichlet or Robin boundary conditions, and $m_{\\rm prior} \\in H^{\\frac{\\alpha}{2}}(\\Omega)$, where $\\Omega \\subset \\mathbb{R}^d$.\n",
    "\n",
    "The parameter $\\alpha > \\frac{d}{2}$ controls the smoothness of the random field and ensures that $\\mathcal{C}_{\\rm prior}$ is a trace class operator (i.e., the infinite sum of the eigenvalues of  $\\mathcal{C}_{\\rm prior}$ is finite). \n",
    "The fact that $\\mathcal{C}_{\\rm prior}$ is trace class is extremely important as it guaratees that the pointwise variance of the samples is finite. (Recall that for a Gaussian random field \n",
    "$ E [\\int_{\\Omega}(m - m_{\\rm prior})^2\\,dx = \\operatorname{trace}(\\mathcal{C}_{\\rm prior})]$).\n",
    "\n",
    "The parameters $\\delta>0$, $\\gamma>0$ can be constant in $\\Omega$ (in this case the prior is called stationary) or spatially varing. It can be shown that the correlation length $\\rho$ of the Gaussian random field is proportional to $\\sqrt{\\frac{\\gamma}{\\delta}}$, while the marginal (pointwise) variance $\\sigma^2$ is proportional to $\\delta^{-2\\alpha}\\rho^{-d}$.\n",
    "\n",
    "In addition, if one wants to introduce anysotropy in the correlation length, one can choose\n",
    "\n",
    "$$ \\mathcal{C}_{\\rm prior} = \\left( \\delta I - \\operatorname{div}(\\Theta \\nabla) \\right)^{-2 \\alpha}, $$\n",
    "\n",
    "where $\\Theta \\in \\mathbb{R}^{d\\times d}$ is a symmetric positive definite tensor, with eigenvalues $\\gamma_i > 0$ ($i=1,\\ldots,d$) and orthonormal eigenvectors $\\boldsymbol{v}_i$ ($i=1,\\ldots,d$). In this case the correlation length in the direction $\\boldsymbol{v}_i$ is proportional to $\\sqrt{\\frac{\\gamma_i}{\\delta}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mesh dependence issues when sampling from a non-trace class operator\n",
    "\n",
    "Here we consider the case when $\\alpha <= \\frac{d}{2}$ and we show that this leads to samples whose stastical properties depends on the spatial discretization.\n",
    "\n",
    "In particular, we take $d=2$ and consider a Laplace-like prior $\\mathcal{C}_{\\rm prior} = \\left( \\delta I - \\gamma \\Delta \\right)^{-1}$, i.e., the case $\\alpha = 1$. \n",
    "\n",
    "We consider three triangulation of the unit square domain $\\Omega = (0,1)^2$: a coarse mesh, a fine mesh, and a locally refined mesh where the mesh size varies. \n",
    "\n",
    "We let $\\delta = 25.$ and $\\gamma = 1.$, so that the correlation length $\\rho$ is of the order of $0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dolfin as dl\n",
    "import numpy as np\n",
    "import math\n",
    "import hippylib as hp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def locallyRefinedMesh():\n",
    "    mesh = dl.UnitSquareMesh(16,16)\n",
    "    ndim = 2\n",
    "    \n",
    "    for i in range(4):\n",
    "        cell_markers = dl.MeshFunction(\"bool\", mesh, ndim)\n",
    "        cell_markers.set_all(False)\n",
    "        for cell in dl.cells(mesh):\n",
    "            if cell.midpoint()[1] < .7 and cell.midpoint()[1] > .3 and cell.midpoint()[0] > .2 and cell.midpoint()[0] < .5:\n",
    "                cell_markers[cell] = True\n",
    "            \n",
    "        mesh = dl.refine(mesh, cell_markers)\n",
    "        \n",
    "    return mesh\n",
    "\n",
    "def generateSamples(mesh, gamma, delta, alpha):\n",
    "    Vh = dl.FunctionSpace(mesh, \"CG\", 1)\n",
    "    if np.abs(alpha - 1.0) < np.finfo(float).eps:\n",
    "        prior = hp.LaplacianPrior(Vh, gamma, delta)\n",
    "        prior.Rsolver = hp.PETScLUSolver(mesh.mpi_comm() )\n",
    "        prior.Rsolver.set_operator(prior.R)\n",
    "    elif np.abs(alpha - 2.0) < np.finfo(float).eps:\n",
    "        prior = hp.BiLaplacianPrior(Vh, gamma, delta)\n",
    "        prior.Asolver = hp.PETScLUSolver(mesh.mpi_comm())\n",
    "        prior.Asolver.set_operator(prior.A)\n",
    "    else:\n",
    "        raise InputError(\"Invalid alpha\")\n",
    "    \n",
    "    noise = dl.Vector()\n",
    "    prior.init_vector(noise, \"noise\")\n",
    "    \n",
    "    sample = dl.Vector()\n",
    "    prior.init_vector(sample, 0)\n",
    "    \n",
    "    ss = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        hp.parRandom.normal(1., noise)\n",
    "        prior.sample(noise, sample)\n",
    "        ss.append(vector2Function(sample, Vh))\n",
    "        \n",
    "    nb.multi1_plot(ss, [\"sample 1\", \"sample 2\", \"sample 3\"])\n",
    "    \n",
    "def pointwiseVariance(mesh, gamma, delta, alpha):\n",
    "    Vh = dl.FunctionSpace(mesh, \"CG\", 1)\n",
    "    if np.abs(alpha - 1.0) < np.finfo(float).eps:\n",
    "        prior = hp.LaplacianPrior(Vh, gamma, delta)\n",
    "        prior.Rsolver = hp.PETScLUSolver(mesh.mpi_comm() ) \n",
    "        prior.Rsolver.set_operator(prior.R)\n",
    "    elif np.abs(alpha - 2.0) < np.finfo(float).eps:\n",
    "        prior = hp.BiLaplacianPrior(Vh, gamma, delta)\n",
    "        prior.Asolver = hp.PETScLUSolver(mesh.mpi_comm()) \n",
    "        prior.Asolver.set_operator(prior.A)\n",
    "    else:\n",
    "        raise InputError(\"Invalid alpha\")\n",
    "    \n",
    "    marginal_variance = prior.pointwise_variance(method=\"Exact\")\n",
    "    \n",
    "    return hp.vector2Function(marginal_variance, Vh)\n",
    "\n",
    "def correlationStructure(prior, center):\n",
    "    rhs = dl.Vector()\n",
    "    prior.init_vector(rhs, 0)\n",
    "    \n",
    "    corrStruct = dl.Vector()\n",
    "    prior.init_vector(corrStruct, 0)\n",
    "    \n",
    "    ps = dl.PointSource(prior.Vh, center, 1.)\n",
    "    ps.apply(rhs)\n",
    "    \n",
    "    prior.Rsolver.solve(corrStruct, rhs)\n",
    "    \n",
    "    return hp.vector2Function(corrStruct, prior.Vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate the three meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh1 = dl.UnitSquareMesh(16,16)\n",
    "mesh2 = dl.UnitSquareMesh(64, 64)\n",
    "mesh3 = locallyRefinedMesh()\n",
    "\n",
    "nb.multi1_plot([mesh1, mesh2, mesh3], [\"Coarse mesh\", \"Fine mesh\", \"Locally refined\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate samples from the Laplacian-like prior ($\\alpha=1$)\n",
    "\n",
    "By visually inspecting a few samples from a Laplacian-like prior in 2D we note that something is not quite right...\n",
    "\n",
    "Samples look rougher where the mesh is finer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.\n",
    "delta = 25.\n",
    "\n",
    "print(\"Coarse mesh\")\n",
    "generateSamples(mesh1, gamma, delta, alpha=1.0)\n",
    "plt.show()\n",
    "print(\"Fine mesh\")\n",
    "generateSamples(mesh2, gamma, delta, alpha=1.0)\n",
    "plt.show()\n",
    "print(\"Locally refined mesh\")\n",
    "generateSamples(mesh3, gamma, delta, alpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pointwise variance for the Laplacian-like  prior ($\\alpha=1$)\n",
    "\n",
    "The pointwise variance is larger when the mesh is finer. In the limit for $h\\rightarrow 0$ the pointwise variance becomes infinite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_1 = pointwiseVariance(mesh1, gamma, delta, alpha=1.)\n",
    "pt_2 = pointwiseVariance(mesh2, gamma, delta, alpha=1.)\n",
    "pt_3 = pointwiseVariance(mesh3, gamma, delta, alpha=1.)\n",
    "\n",
    "print(\"Pointwise variance\")\n",
    "nb.multi1_plot([pt_1, pt_2, pt_3], [\"Coarse Mesh\", \"Fine Mesh\", \"Refined Mesh\"], same_colorbar=True, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples from Bilaplacian-like prior ($\\alpha=2$)\n",
    "\n",
    "Samples look qualitatively the same on different meshes. In fact, $\\alpha=2$ is sufficient in 2D for defining a well-behaved prior and well-posed inverse problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.\n",
    "delta = 25.\n",
    "\n",
    "print(\"Coarse mesh\")\n",
    "generateSamples(mesh1, gamma, delta, alpha=2.0)\n",
    "plt.show()\n",
    "print(\"Fine mesh\")\n",
    "generateSamples(mesh2, gamma, delta, alpha=2.0)\n",
    "plt.show()\n",
    "print(\"Locally refined mesh\")\n",
    "generateSamples(mesh3, gamma, delta, alpha=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise variance for the Bilaplacian-like  prior ($\\alpha=2$)\n",
    "\n",
    "The pointwise variance is independent of the mesh resolution. This is because $\\alpha = 2$ gives a covariance belonging to the space of trace-class operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_1 = pointwiseVariance(mesh1, gamma, delta, alpha=2.)\n",
    "pt_2 = pointwiseVariance(mesh2, gamma, delta, alpha=2.)\n",
    "pt_3 = pointwiseVariance(mesh3, gamma, delta, alpha=2.)\n",
    "\n",
    "print(\"Pointwise variance\")\n",
    "nb.multi1_plot([pt_1, pt_2, pt_3], [\"Coarse Mesh\", \"Fine Mesh\", \"Refined Mesh\"], same_colorbar=True, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary artifacts\n",
    "\n",
    "The above figure shows that the pointwise variance is larger close to the boundary of the domain than in the interior. This boundary artifact is due to the use on natural (homogeneous Neumann) boundary condition. Similar artifacts occur if one uses homogeneous Dirichlet boundary conditions.\n",
    "\n",
    "To mitigate these boundary effects one could instead consider Robin boundary conditions of the form\n",
    "\n",
    "$$ \\nabla m \\cdot \\boldsymbol{n} + \\beta m = 0 \\text{ on } \\partial\\Omega, $$\n",
    "\n",
    "where the value $\\beta = \\frac{\\sqrt{\\gamma\\delta}}{1.42}$ has empirically be found to be a good choice.\n",
    "\n",
    "The figure below show the pointwise variance using both natural and Robin boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.\n",
    "delta = 25.\n",
    "\n",
    "mesh = dl.UnitSquareMesh(32,32)\n",
    "Vh = dl.FunctionSpace(mesh, \"CG\", 1)\n",
    "\n",
    "prior_natural = BiLaplacianPrior(Vh, gamma, delta,  robin_bc=False)\n",
    "prior_robin = BiLaplacianPrior(Vh, gamma, delta,  robin_bc=True)\n",
    "\n",
    "\n",
    "## pointwise variance \n",
    "pointwise_variance_natural = vector2Function(prior_natural.pointwise_variance(method=\"Exact\"), Vh)\n",
    "pointwise_variance_robin = vector2Function(prior_robin.pointwise_variance(method=\"Exact\"), Vh)\n",
    "\n",
    "print(\"Pointwise variance\")\n",
    "nb.multi1_plot([pointwise_variance_natural, pointwise_variance_robin], [\"Natural BC\", \"Robin BC\"],\n",
    "               same_colorbar = True, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anysotropic correlation lengths\n",
    "$\\newcommand{\\prcov}{\\mathcal{C}_{\\rm prior}}$\n",
    "\n",
    "Here we assume a Gaussian prior, $\\mu_{\\rm prior} \\sim \\mathcal{N}(0, \\prcov)$ with zero mean and covariance matrix $\\prcov = \\mathcal{A}^{-2}$, where $\\mathcal{A}$ is a differential operator of the form\n",
    "\n",
    "$$ \\mathcal{A} = -\\gamma {\\nabla\\cdot}\\, \\left(\\Theta\\, {\\nabla}\\right) + \\delta I. $$\n",
    "\n",
    "Here $\\Theta$ is a s.p.d. anisotropic tensor of the form\n",
    "\n",
    "$$ \\Theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_1 \\sin^2\\alpha & (\\theta_1-\\theta_2) \\sin\\alpha\\, \\cos{\\alpha} \\\\\n",
    "(\\theta_1-\\theta_2) \\sin\\alpha\\, \\cos{\\alpha} & \\theta_2 \\cos^2\\alpha.\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This allows to generate distributions that have large correlation lenght in some directions, which can be useful in prior modeling. Below we show samples from the resulting distribution, the pointwise variance as well as a covariance function for the center of the domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.\n",
    "delta = 25.\n",
    "\n",
    "mesh = dl.UnitSquareMesh(64,64)\n",
    "Vh = dl.FunctionSpace(mesh, \"CG\", 1)\n",
    "    \n",
    "anis_diff = dl.Expression(code_AnisTensor2D, degree=1)\n",
    "anis_diff.theta0 = 2.\n",
    "anis_diff.theta1 = .5\n",
    "anis_diff.alpha = math.pi/4\n",
    "prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff, robin_bc=True)\n",
    "\n",
    "## Generate and show 6 samples\n",
    "noise = dl.Vector()\n",
    "prior.init_vector(noise, \"noise\")\n",
    "    \n",
    "sample = dl.Vector()\n",
    "prior.init_vector(sample, 0)\n",
    "    \n",
    "ss = []\n",
    "    \n",
    "for i in range(6):\n",
    "    parRandom.normal(1., noise)\n",
    "    prior.sample(noise, sample)\n",
    "    ss.append(vector2Function(sample, Vh))\n",
    "        \n",
    "nb.multi1_plot(ss[0:3], [\"sample 1\", \"sample 2\", \"sample 3\"])\n",
    "nb.multi1_plot(ss[3:6], [\"sample 4\", \"sample 5\", \"sample 6\"])\n",
    "plt.show()\n",
    "\n",
    "## pointwise variance and correlation structure\n",
    "\n",
    "pointwise_variance = vector2Function(prior.pointwise_variance(method=\"Randomized\", r=200), Vh)\n",
    "correlation_struc  = correlationStructure(prior, dl.Point(.5,.5))\n",
    "\n",
    "nb.multi1_plot([pointwise_variance, correlation_struc], [\"Pointwise variance\", \"Correlation Structure\"],\n",
    "               same_colorbar = True, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Copyright &copy; 2019-2020, Washington University in St. Louis.\n",
    "\n",
    "All Rights reserved.\n",
    "See file COPYRIGHT for details.\n",
    "\n",
    "This file is part of **cmis_labs**, the teaching material for  ESE 5932 *Computational Methods for Imaging Science* at Washington University in St. Louis. Please see [https://uvilla.github.io/cmis_labs](https://uvilla.github.io/cmis_labs) for more information and source code availability.\n",
    "\n",
    "We would like to acknowledge the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1548562, for providing cloud computing resources (Jetstream) for this course through allocation TG-SEE190001."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
